# 机器学习和向量化
## Machine Learning & Vectorization

## Gensim Word2Vec
Word2Vec的算法的核心思想是：如果两个单词具有非常相似的上下文，那么这些单词在含义上可能非常相似或者至少是相关的。例如，`shocked`，`appalled` 和 `astonished` 的词语通常用于类似的语境中。

```
一个词的含义可以由它的上下文推断出来
```

根据这个基本假设，可以使用Word2Vec来计算相似概念、找到不相关的概念、计算两个单词之间的相似性等等。

### Gensim库
我们使用Word2Vec的Gensim库来实现，计算过程包括2个步骤：

- 输入数据
- 参数设置

### 包含库以及日志
首先，我们从导入库开始并建立日志记录：
```python
import gzip
import gensim 
import logging
logging.basicConfig(format=’%(asctime)s : %(levelname)s : %(message)s’, level=logging.INFO)
```
 

### 数据集
Word2Vec需要大量的文本数据作为训练集合，本教程将使用OpinRank数据集中的数据，它包括汽车和酒店的用户评论，文件的每一行代表酒店评论，该文件压缩后大约97MB。

首先打印第一行数据看看
```python
with gzip.open (input_file, 'rb') as f:
        for i,line in enumerate (f):
            print(line)
            break

 
b"Oct 12 2009 \tNice trendy hotel location not too bad.\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\t\r\n"
```

这是一个非常好的完整评论，包括很多单词，这就是我们想要的；此数据集中有大约255,000条此类评论。

Gensim的Word2Vec要求传递一个标记化句子列表作为输入，但是如果数据之间的区别不大的话，实际上可以将整个评论作为一个句子输入。最后，可以使用数据集计算输入词的所有相邻词（上下文）。

### 将文件读入列表
首先从压缩文件中将数据读入列表，并且对评论进行预处理`gensim.utils.simple_preprocess (line)`，例如`标记化`、`转换小写`等，并返回一个标记列表。

```python
def read_input(input_file):
    """This method reads the input file which is in gzip format"""
    logging.info("reading file {0}...this may take a while".format(input_file))
    with gzip.open(input_file, 'rb') as f:
        for i, line in enumerate(f):
            if (i % 10000 == 0):
                logging.info("read {0} reviews".format(i))
            # do some pre-processing and return list of words for each review text
            yield gensim.utils.simple_preprocess(line)
```

### 训练Word2Vec模型
训练模型非常简单。您只需实例化Word2Vec并传递我们在上一步中阅读的评论。所以，我们实际上是在传递列表列表。主列表中的每个列表都包含一组来自用户评论的标记。Word2Vec使用所有这些令牌在内部创建词汇表。通过词汇，我的意思是一组独特的单词。

```python
# build vocabulary and train model
    model = gensim.models.Word2Vec(
        documents,
        size=150,
        window=10,
        min_count=2,
        workers=10)
    model.train(documents, total_examples=len(documents), epochs=10)
```

在构建词汇表之后，我们只需要调用`train(...)`即可开始训练Word2Vec模型，即训练一个具有单个隐藏层的神经网络，训练模型根据上下文预测当前单词（使用默认的神经结构），计算目标是学习隐藏层的权重；这些权重本质上是单词特征向量，单词特征向量可以描述目标词的一些功能，例如`king`可以用`性别`、`年龄`、`国王`等来描述。


### 计算相似度
第一个示例显示了与“dirty”一词类似的单词查找, 调用`most_similar`函数返回前10个相似的单词。

```python
In [12]:w1 = "dirty"
        model.wv.most_similar (positive=w1)
Out[12]:[('filthy', 0.871721625328064),
         ('stained', 0.7922376990318298),
         ('unclean', 0.7915753126144409),
         ('dusty', 0.7772612571716309),
         ('smelly', 0.7618112564086914),
         ('grubby', 0.7483716011047363),
         ('dingy', 0.7330487966537476),
         ('gross', 0.7239381074905396),
         ('grimy', 0.7228356599807739),
         ('disgusting', 0.7213647365570068)]
```


```python
In [50]:
# look up top 6 words similar to 'polite'
w1 = ["polite"]
     model.wv.most_similar (positive=w1,topn=6)
Out[50]: [('courteous', 0.9174547791481018),
          ('friendly', 0.8309274911880493),
          ('cordial', 0.7990915179252625),
          ('professional', 0.7945970892906189),
          ('attentive', 0.7732747197151184),
          ('gracious', 0.7469891309738159)]
```

```python
In [53]:
# look up top 6 words similar to 'france'
w1 = ["france"]
     model.wv.most_similar (positive=w1,topn=6)
Out[53]: [('canada', 0.6603403091430664),
          ('germany', 0.6510637998580933),
          ('spain', 0.6431018114089966),
          ('barcelona', 0.61174076795578),
          ('mexico', 0.6070996522903442),
          ('rome', 0.6065913438796997)]
```

```python
In [54]:
# look up top 6 words similar to 'shocked'
w1 = ["shocked"]
     model.wv.most_similar (positive=w1,topn=6)
Out[54]: [('horrified', 0.80775386095047),
          ('amazed', 0.7797470092773438),
          ('astonished', 0.7748459577560425),
          ('dismayed', 0.7680633068084717),
          ('stunned', 0.7603034973144531),
          ('appalled', 0.7466776371002197)]
```

### 上下文搜索
我们甚至可以指定几个正面示例来查找上下文中相关的内容，并提供负面示例来说明不应被视为相关的内容。 在下面的示例中，我们查找所有与bed相关的数据：
```python
In [55]: # get everything related to stuff on the bed
         w1 = ["bed",'sheet','pillow']
         w2 = ['couch']
         model.wv.most_similar (positive=w1,negative=w2,topn=10)
Out[55]: [('duvet', 0.7086508274078369),
          ('blanket', 0.7016597390174866),
          ('mattress', 0.7002605199813843),
          ('quilt', 0.6868821978569031),
          ('matress', 0.6777950525283813),
          ('pillowcase', 0.6413239240646362),
          ('sheets', 0.6382123827934265),
          ('foam', 0.6322235465049744),
          ('pillows', 0.6320573687553406),
          ('comforter', 0.5972476601600647)]
```

### 单词比较
甚至可以使用Word2Vec通过调用`similarity(...)`函数并传入相关单词来计算词汇表中两个单词之间的相似性。
```python
In [57]: # similarity between two different words
         model.wv.similarity(w1="dirty",w2="smelly")
Out[57]: 0.76181122646029453
In [58]: # similarity between two identical words
         model.wv.similarity(w1="dirty",w2="dirty")
Out[58]: 1.0000000000000002
In [59]: # similarity between two unrelated words
         model.wv.similarity(w1="dirty",w2="clean")
Out[59]:0.25355593501920781
```

上述三个片段使用每个单词的矢量计算两个指定单词之间的余弦相似度, 从上面的结果数值看，`dirty`与`smelly`相似，但`dirty`与`clean`不相似。


### 查找非同类数据
您甚至可以使用Word2Vec查找给定项目列表中不属于该分类的项目。

```python
In [63]: # Which one is the odd one out in this list?
         model.wv.doesnt_match(["cat","dog","france"])
Out[63]: 'france'
In [77]: # Which one is the odd one out in this list?
         model.wv.doesnt_match(["bed","pillow","duvet","shower"])
Out[77]: 'shower'
```

### 参数设置
为了更好地训练模型，必须了解Word2Vec函数的参数含义。

```python
model = gensim.models.Word2Vec(documents, size=150, window=10, min_count=2, workers=10)
```

- size: 用于表示每个标记或单词（即上下文或相邻单词）的密集向量的大小。如果您的数据有限，那么大小应该是一个小得多的值，因为您只有给定单词的这么多唯一邻居。如果您有大量数据，那么可以尝试各种尺寸。对于相似性查找，值100-150对我来说效果很好。
- window: 目标字与其相邻字之间的最大距离。如果邻居的位置大于左侧或右侧的最大窗口宽度，则某些邻居不会被视为与目标字相关。理论上，较小的窗口应该为您提供更相关的术语。同样，如果您的数据不稀疏，那么窗口大小不应该太大，只要它不过于狭窄或过宽。如果您对此不太确定，请使用默认值。
- min_count: 字的最小频率计数。该模型会忽略不满足的词min_count。非常罕见的单词通常不重要，所以最好摆脱那些。除非您的数据集非常小，否则这不会影响模型的最终结果。此处的设置可能会对模型文件的内存使用和存储要求产生更多影响。
- workers: 在幕后使用多少个线程？

### 什么时候应该使用Word2Vec？
Word2Vec有许多应用场景。想象一下，如果你需要建立一个情感词典。在大量用户评论上训练Word2Vec模型可帮助您实现这一目标。你的词汇不仅仅是情感，而是词汇中的大部分词汇。

除了原始的非结构化文本数据，您还可以使用Word2Vec获取更多结构化数据。例如，如果您有一百万个stackoverflow问题和答案的标签，您可以找到相关标签并推荐那些用于探索。您可以通过将每组共同标记视为“句子”并在此数据上训练Word2Vec模型来实现此目的。当然，您仍需要大量示例才能使其正常运行。


## How to generate embeddings of phrases –Phrase2Vec

使用`短语训练`与使用`单词训练`模型非常相似，不同之处在于：在处理文本数据时预先处理短语。在本教程中，本文介绍如何使用短语训练方法，不需要明确指定构成短语的单词数（即n-gram大小），这意味着可以拥有包含2或3个单词的短语，甚至包含4或5个短语。

算法步骤为：

- 第1步：查找语料库中的常用短语
- 第2步：使用短语标记语料库
- 第3步：使用新发现的短语训练Word2Vec模型

### 第1步：查找语料库中的常用短语
分析短语的第一步是识别构成短语的单词组。有很多方法可以识别短语，比如NLTK方法通过“分块”的语言重型方法来检测短语，它使用一组分隔符标记对整个文本语料库进行分段，分隔符可以是特殊字符、停用词和指示短语边界的其他术语。

停用单词非常适合将文本拆分为一组短语，因为它们通常是`连接词`和`填充词`，以便制作一个清晰，详细的句子。您可以获得创意并使用更完整的停用词列表，或者您甚至可以过度简化此列表，使其成为最小的停用词列表。

下面的代码向您展示了如何使用特殊字符和停止单词将文本分成一组候选短语。
```python
def generate_candidate_phrases(text, stopwords):
    """ generate phrases using phrase boundary markers """
    # generate approximate phrases with punctation
    coarse_candidates = char_splitter.split(text.lower())
    candidate_phrases = []
    for coarse_phrase\
            in coarse_candidates:
        words = re.split("\\s+", coarse_phrase)
        previous_stop = False
        # examine each word to determine if it is a phrase boundary marker or part of a phrase or lone ranger
        for w in words:
            if w in stopwords and not previous_stop:
                # phrase boundary encountered, so put a hard indicator
                candidate_phrases.append(";")
                previous_stop = True
            elif w not in stopwords and len(w) > 3:
                # keep adding words to list until a phrase boundary is detected
                candidate_phrases.append(w.strip())
                previous_stop = False
    # get a list of candidate phrases without boundary demarcation
    phrases = re.split(";+", ' '.join(candidate_phrases))
    return phrases
```

在上面的代码中，我们首先使用逗号，句点和分号等特殊字符将文本拆分为粗粒度单位。然后使用停用词进行更精细的边界检测。当您对语料库中的所有文档或句子重复此过程时，您将最终得到一大堆短语。然后，可以使用频率计数和其他度量（如Pointwise Mutual Information）来显示顶部短语，这些度量可以衡量短语中单词之间的关联强度。对于短语嵌入任务，我们必须使用大量的数据，因此单独的频率计数就足以完成这项任务。在其他一些任务中，我将频率计数与Pointwise Mutual Information相结合，以获得更好的测量短语质量。

为了确保可扩展性，可以使用Spark，因为您可以在一台计算机上利用其内置的多线程功能，或者如果您确实需要处理大量数据，则可以使用多台计算机来获得更多的CPU能力。下面的代码显示了PySpark方法，它读取文本文件，清理它，生成候选短语，计算短语的频率，并将其过滤到一组满足最小频率计数的短语。在本地运行的450 MB数据集上，这需要大约一分钟来发现顶级短语，并且需要7分钟来使用短语来注释整个文本语料库。您可以按照千篇一词的 repo中的说明使用此PySpark代码来发现数据的短语。
```python
def generate_and_tag_phrases(text_rdd,min_phrase_count=50):
    """Find top phrases, tag corpora with those top phrases"""
 
    # load stop words for phrase boundary marking
    logging.info ("Loading stop words...")
    stopwords = load_stop_words ()
 
    # get top phrases with counts > min_phrase_count
    logging.info ("Generating and collecting top phrases...")
    top_phrases_rdd = \
        text_rdd.map(lambda txt: remove_special_characters(txt))\
        .map(lambda txt: generate_candidate_phrases(txt, stopwords)) \
        .flatMap(lambda phrases: phrase_to_counts(phrases)) \
        .reduceByKey(add) \
        .sortBy(lambda phrases: phrases[1], ascending=False) \
        .filter(lambda phrases: phrases[1] >= min_phrase_count) \
        .sortBy(lambda phrases: phrases[0], ascending=True) \
        .map(lambda phrases: (phrases[0], phrases[0].replace(" ", "_")))
 
    shortlisted_phrases = top_phrases_rdd.collectAsMap()
    logging.info("Done with phrase generation...")
 
    # write phrases to file which you can use down the road to tag your text
    logging.info("Saving top phrases to {0}".format(phrases_file))
    with open(os.path.join(abspath, phrases_file), "w") as f:
        for phrase in shortlisted_phrases:
            f.write(phrase)
            f.write("\n")
 
    # tag corpora and save as new corpora
    logging.info("Tagging corpora with phrases...this will take a while")
    tagged_text_rdd = text_rdd.map(
            lambda txt: tag_data(
                txt,
                shortlisted_phrases))
 
    return tagged_text_rdd
```
以下是使用上述代码在餐馆评论数据集中找到的短语的微小快照。



### 第2步：使用短语标记语料库
有两种方法可以将某些单词标记为语料库中的短语。一种方法是预先注释整个语料库并生成一个新的“带注释的语料库”。另一种方法是在学习嵌入之前在预处理阶段注释您的句子或文档。有一个单独的注释层可以更清晰，不会干扰训练阶段。否则，由于训练或注释，您的模型是否很慢将更难判断。

在注释你的语料库时，你需要做的就是以某种方式加入构成短语的单词。对于此任务，我只使用下划线来加入单个单词。所以，“…ate fried chicken and onion rings…”会变成“…ate fried_chicken and onion_rings…”

### 第3步：使用Word2Vec训练Phrase2Vec模型
一旦在语料库中明确标记了短语，训练阶段就与使用Gensim或任何其他库的任何Word2Vec模型完全相似。您可以按照我的Word2Vec Gensim教程获取有关如何训练和使用Word2Vec的完整示例。

### 短语嵌入的示例用法
下面的示例显示了用于查找类似概念时短语嵌入的强大功能。这些是来自餐厅领域的概念，使用Gensim对450 MB的餐厅评论进行了培训。

类似和相关的unigrams，bigrams和trigrams
请注意，我们能够捕获高度相关的概念，即unigrams，bigrams和更高阶的n-gram。

```bash
# 最类似于'green_curry'：
--------------------------------------
（'panang_curry'，0.8900948762893677）
（'yellow_curry'，0.884008526802063）
（'panang'，0.8525004386901855）
（'drunken_noodles'，0.850254237651825）
（'basil_chicken'，0.8400430679321289）
（'coconut_soup'，0.8296557664871216）
（'massaman_curry'，0.827597975730896）
（'pineapple_fried_rice'，0.8266736268997192）


# 最类似于'singapore_noodles'：
--------------------------------------
（'shrimp_fried_rice'，0.7932361960411072）
（'drunken_noodles'，0.7914629578590393）
（'house_fried_rice'，0.7901676297187805）
（'mongolian_beef'，0.7796567678451538）
（'crab_rangoons'，0.773795485496521）
（'basil_chicken'，0.7726351022720337）
（'crispy_beef'，0.7671589255332947）
（'steamed_dumplings'，0.7614079117774963）


# 最类似'chicken_tikka_masala'：
--------------------------------------
（'korma'，0.8702514171600342）
（'butter_chicken'，0.8668922781944275）
（'tikka_masala'，0.8444720506668091）
（'garlic_naan'，0.8395442962646484）
（'lamb_vindaloo'，0.8390569686889648）
（'palak_paneer'，0.826908528804779）
（'chicken_biryani'，0.8210495114326477）
（'saag_paneer'，0.8197864294052124）


# 最类似于'breakfast_burrito'：
--------------------------------------
（'huevos_rancheros'，0.8463341593742371）
（'huevos'，0.789624035358429）
（'chilaquiles'，0.7711247801780701）
（'breakfast_sandwich'，0.7659544944763184）
（'rancheros'，0.7541004419326782）
（'omelet'，0.7512155175209045）
（'scramble'，0.7490915060043335）
（'omlet'，0.747859001159668）

# 最类似于'little_salty'：
--------------------------------------
（'little_bland'，0.745500385761261）
（'little_spicy'，0.7443351149559021）
（'little_oily'，0.7373550534248352）
（'little_overcooked'，0.7355216145515442）
（'kinda_bland'，0.7207454442977905）
（'slightly_overcooked'，0.712611973285675）
（'little_greasy'，0.6943882703781128）
（'cooked_nicely'，0.6860566139221191）


# 最类似于'celiac_disease'：
--------------------------------------
（'celiac'，0.8376057744026184）
（'不容忍'，0.7442486882209778）
（'gluten_allergy'，0.7399739027023315）
（'celiacs'，0.7183824181556702）
（'不宽容'，0.6730632781982422）
（'gluten_free'，0.6726624965667725）
（'food_allergies'，0.6587174534797668）
（'gluten'，0.6406026482582092）
```

### 类似的概念表达不同
在这里，您将看到可以捕获以不同方式表达的类似概念。
```bash
# 最类似于'rationalably_priced'：
--------------------------------------
（'quite_priced'，0.8588327169418335）
（'实惠'，0.7922118306159973）
（'便宜'，0.7702735066413879）
（'decently_priced'，0.7376087307929993）
（'rational_priced'，0.7328246831893921）
（'pricing_reasonably'，0.6946456432342529）
（'pricing_right'，0.6871092915534973）
（'moderately_priced'，0.6844340562820435）


# 最类似于'highly_recommend'：
--------------------------------------
（'definitely_recommend'，0.9155156016349792）
（'strong_recommend'，0.86533123254776）
（'absolute_recommend'，0.8545517325401306）
（'totally_recommend'，0.8534528017044067）
（'推荐'，0.8257364630699158）
（'sure_recommend'，0.785507082939148）
（'highly_reccomend'，0.7751532196998596）
（'highly_recommended'，0.7553941607475281）
```

### 摘要
总之，要生成短语嵌入，您需要在训练Word2Vec模型之前为短语发现添加一个图层。如果您拥有大量数据，则文本数据挖掘方法具有轻量级和可扩展性的优势，而不会影响质量。此外，您不必提前指定短语大小或受特定词汇限制。语言重型方法在词性和您正在处理的短语类型（例如，名词短语与动词短语）方面提供了更多的特异性。如果您真的需要这些信息，那么您可以考虑采用文本挖掘方法的分块方法。
