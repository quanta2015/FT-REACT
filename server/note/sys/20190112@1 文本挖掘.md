# Text Mining & NLP Basics
## 文本相似

### 什么是文本相似性？
在谈论文本相似性时，不同的人对文本相似性的含义略有不同。本质上，目标是计算两个文本`含义接近的程度`，或者`表面接近程度`。第一种称为`语义相似性`，后者称为`词汇相似性`。 尽管`词汇相似性`的方法，通常用于实现`语义相似性`（在一定程度上），但实现真正的`语义相似性`通常更为复杂。 


### 词汇或词汇级相似度
在大多数情况下，当提到`文本相似性`时，人们实际上指的是两个文本在表面上的相似程度。例如，从单词角度来说，`the cat ate the mouse`与`the mouse ate the cat food`非常相似，因为4个单词中的3个是精确重叠。

这种相似性概念通常被称为`词汇相似性`。它通常不考虑单词背后的实际含义或上下文中的整个短语。虽然这些短语的实际含义经常被忽略，但这并不意味着以这种方式计算相似性是无效的。实际上，您可以提出创造性的方法来扩展每个单词或一组相关单词（词汇链）的范围，以提高被比较的两个文本之间的相似性。例如，如果您要比较报纸文章中的短语之间的相似性。您可以在短语中的当前单词的左侧和右侧使用N个非限定词，以进行简单的范围扩展。您实际上提供了更多的上下文，而不是单词进行单词比较。这类似于扩展搜索查询。

### 粒度
另一点需要注意的是，可以以各种粒度计算词汇相似度。您可以在`字符级别`，`单词级别`或`短语级别`计算词汇相似度，在计算相似度之前将一段文本分成一组相关单词。字符级别相似性​​也称为字符串相似性匹配，通常用于确定两个字符串的接近程度。例如，名字'Kavita Ganesan'和'Kavita A Ganesan'的非常接近！我们可以使用下面算法来计算字符串相似性，也可以使用距离的度量来量化两个字符串的不同之处。

```
Overlap = 'cat ate mouse' ^ 'mouse ate cat food' = 3
```

### 常用指标
用于计算两段文本之间相似性的一些最常见的度量标准是`Jaccard系数`，`Dice`和`余弦相似性`。以下是计算`Jaccard`的方法：

<img src="http://latex.codecogs.com/gif.latex?Jaccard= \frac {| tokens\_in\_string\_A \bigcap tokens\_in\_string\_B |} {| tokens\_in\_string\_A \bigcup tokens\_in\_string\_B |}">

这些都是集合的集合除以集合的集合，其结果值将介于[0,1]之间，可以根据需要设置阈值。我发现有效的短语（最大长度为10个字）阈值一般大于0.6。对于较长的文本，此值可能较小，或者如果只关心边际重叠，则此值可能会更小。正常化的预处理和过滤文本（例如提取词干，去除噪音，删除停用词）越多，使用简单度量（如Jaccard）的文本相似性度量的结果就越好。

### 词汇相似性在哪里使用？
- 聚类: 将类似的文本组合在一起，判断两组文本是否相似？
- 冗余删除: 判断两段文本是否相似，删除冗余的数据；比如整理重复的产品列表，或数据库中删除同名用户，或者整理重复的html页面。
- 信息检索: 使用更成熟的信息检索算法，如`BM25`，`PL2` 等，也可以使用余弦（对于较长文本）或`jaccard`和`dice`（较短文本）等度量。

### 语义相似性
上面讨论了词汇相似性，而目前的主流是研究两个短语的含义是多么相似？如果我们看一下这些短语，  `the cat ate the mouse` 和 `the mouse ate the cat food` ，虽然词语明显重叠，但实际上有不同的含义。从短语中获取意义通常是一项更困难的任务，因为它需要更深层次的分析。在这个例子中，如单词的顺序：`cat => ate => mouse`和`mouse => ate => cat food`。虽然单词重叠，但单词顺序的不同，决定了两个短语具有不同的含义。这只是一个简单的例子。大多数人使用语法分析来帮助语义相似性。让我们看看这两个短语的解析树。你能从中得到什么？

![](img/TextSimilarity1.png)

![](img/TextSimilarity2.png)

你可以得到的短语的解析(例如`cat food`)，依赖结构(如在第一种情况下，`mouse`是 `ate`的`object` ，在第二种情况下，`food`是`ate`的`object`)，以及词的类型(名词，动词，形容词等)，所有这些都可以以不同的方式用于评估语义相似性。语义相似性通常用于解决NLP任务，例如复述识别和自动问答。



## 关于文本挖掘和信息检索的停止词

### stop word list
在处理文本挖掘应用程序时，我们经常听到 `stop words` 或 `stop word list` 或 `stop list` 这一术语。停用词基本上是一组任何语言中常用的单词，而不仅仅只是英语。停用词对许多应用程序至关重要的原因是，如果我们删除在给定语言中非常常用的单词，我们就可以`专注于重要单词`。例如，在搜索引擎的上下文中，如果您的搜索查询是` how to develop information retrieval applications `，那么搜索引擎会尝试查找包含术语 `how`, `to` `develop`, `information`, `retrieval`, `applications` ，其中术语`how`和`to`在英语中太常用就导致语意泛化。因此，如果我们必须忽略这两个术语，搜索引擎实际上可以专注于检索包含关键字的页面：`develop`，`information`，`retrieval`，`applications` - 这将更贴切用户真正感兴趣的页面。

停用词可用于很多方向，比如：
- 有监督的机器学习 - 从特征空间中删除停用词
- 群集 - 在生成群集之前删除停用词
- 信息检索 - 防止停用词被索引 
- 文本摘要 - 在计算ROUGE分数时排除停用词对摘要分数的贡献和删除停用词

### 停用词的类型
停止词一般被认为是`单一词汇`。它对不同的应用程序来说真的意味着不同 例如，某些应用需要删除定量词（a，an）、介词（above, across, before）、形容词（good, nice）。但是对于某些应用，这可能是有害的。例如，在情绪分析中，删除诸如`good`和`nice`之类的形容词术语以及诸如`not`之类的否定可以使算法出错。在这种情况下，可以选择使用最小停止列表，该列表仅由具有`介词`或`定量词`组成，或者仅根据实际需要设定。

最小停用词列表的示例：
- 定量词：标记名词并且跟在名词前出现，比如：a，a，an，another
- 协调连词：协调连词连接单词，短语和子句，比如：for，an，nor，but，or，so，so
- 介词：介词表达时间或空间关系，比如：in，under，towards，before

在某些特定领域的案例中，例如临床文本，我们可能需要一整套不同的停用词。例如，与`heart``failure`和`diabetes`等术语相比，像`mcg` `dr`和`patient`这样的术语在构建智能应用程序时可能具有较少的区分能力。在这种情况下，我们还可以构建特定于域的停用词，而不是使用已发布的停用词列表。


### 停止短语
停止短语就像停止单词一样，不是删除单个单词，而是排除短语。例如，如果短语`good item`非常频繁地出现，导致结果中出现不需要的特征值，则可以选择添加诸如停用短语，`stop phrases`的构造方法和停用词一样。例如，您可以将语料库中出现次数很少的短语视为短语，也可以将语料库中几乎每个文档中出现的短语视为停用短语。 


### 已发布的停止词列表
如果您想使用停用词，那么您可以使用以下列表：

-  [雪球停止单词列表](http://snowball.tartarus.org/algorithms/english/stop.txt) : Snowball Stemmer发布的单词列表。
- [梗犬停止词列表](https://bitbucket.org/kganes2/text-mining-resources/downloads/): Terrier包发布的非常全面的停用词列表。
- [最小停止词列表](https://bitbucket.org/kganes2/text-mining-resources/downloads/): 这是我编译的停用词列表，由确定器，协调连词和介词组成
- 构建自己的停用词列表: 构建自己的停用词数据库


### 构建特定于域的停用词列表
虽然使用已发布的一组停用词相当容易，但在许多情况下，使用这样的停用词对于某些应用来说是完全不够的。例如，在临床文本中，`mcg`，`dr。`和`patient`等术语几乎出现在您遇到的每个文档中。因此，这些术语可被视为临床文本挖掘和检索的潜在停用词。类似地，对于推文，诸如`＃``RT`，`@ username`之类的术语可能被视为停用词。通用语言特定停用词列表通常不涵盖此类域特定术语。


## 构建自定义停用词列表的提示
停用词是任何语言中常用词汇的集合。例如，在英语中，`the`，`is`和`and`很容易被认定为停用词。在NLP和文本挖掘应用程序中，停用词用于消除不重要的单词，允许应用程序专注于重要单词。

虽然使用国际公共组织发布的一组停用词相当容易，但在许多情况下，使用这样的停用词对于某些应用来说是完全无法帮助我们解决问题的。例如，在临床文本中，`mcg`，`dr。`和`patient`等术语几乎出现每个文档中。因此，这些术语可被视为临床文本挖掘和检索的潜在停用词。类似地，对于推文，诸如`＃`、`RT`、`@username`之类的术语可能被视为停用词。通用停用词通常不能涵盖特定领域的术语。

事实上，构建自己的特定于域的停用词列表实际上相当容易。下面有几种方法都可以构建停用词：

### 1. 最常见的术语是停用词
在集合中的所有文档中汇总每个唯一单词的术语频率，按原始术语频率的降序对术语进行排序，然后选取前N个术语作为停止词；同时还可以在排序之前删除常用英语单词（使用停止列表），以确保定位特定于域的停用词；另一种选择是将更多X％文档中出现的单词视为停用词。我个人发现，消除85％的文档中出现的单词在几个文本挖掘任务中都是有效的。这种方法的好处在于它实际上很容易实现，但缺点是如果你有一个特别长的文档，只需几个文档的原始术语频率可以占主导地位，并导致该术语位于顶部。

### 2.最不常见的术语作为停用词
正如非常频繁的术语可能会分散术语而不是区分术语一样，非常罕见的术语也可能对文本挖掘和检索没有用处。例如，在推文集合中只出现一次的用户名`@username`, 像`yoMateZ！`的编造术语，对文本可能没用。请注意，某些术语如`yaaaaayy !!`通常可以转换为标准形式`yay`，尽管规范化了，如果术语频率仍为1，则可以将其删除，这可以显着减少数据量。

### 3.低IDF条款作为停用词
反向文档频率（IDF）基本上是指集合中包含特定术语`ti`的文档的反向部分。我们假设你有N个文件，并且术语`ti`出现在N个文档的M中。因此，`ti`的`IDF`计算如下：

```
IDF(ti)= Log N / M.
```

因此，出现的文件越多，`IDF`得分越低，这意味着每个文档中出现的术语的`IDF`分数为0。如果按照`IDF`分数降序排列集合中的每个`ti`，可以将`IDF`分数最低的底部K组术语视为停止词。同样，我们可以在排序之前删除常用英语单词（已发布的停止列表），以确保定位特定于域的低`IDF`单词。如果K足够大，它将修正一般停用词和域特定停用词。

### 测试停用词
那么你怎么知道删除特定领域的停用词对实际情况有帮助？我们可以在数据子集上进行测试，查看精度和性能的衡量标准是否有所改善，保持不变或降低；如果它降低了，说明这些停用词无效。



## IDF算法 - 词汇概率计数
`n-gram` 或 `单词概率`说明了单词对文档的重要性，但是由于文档几乎总是具有 `连词`、`确定符`以及其他类型的`干扰单词`，因此使用原始单词的概率效果都很差。一般我们使用更大的集合来规范化这些概率。比如，如果目前有3个文档，并且我们希望在文档中找到最重要的单词，文档如下所示：

Doc1 { the cat and dog and mouse ate cheese }  
Doc2 { the mouse in the house was lovely }  
Doc3 { the mouse in the jungle was dead }  

> 现在，让我们看一下单词计数。
```bash
# 整个集合（即Doc1 + Doc2 + Doc3）：
{
  "results": [
    " the:5",
    " mouse:3",
    " in:2",
    " and:2",
    " was:2",
    " house:1",
    " jungle:1",
    " cat:1",
    " lovely:1",
    " ate:1",
    " cheese:1",
    " dead:1",
    " dog:1"
  ]
}
```

Doc1的计数：
```bash
Doc 1 {
  "results": [
    " and:2",
    " mouse:1",
    " cat:1",
    " ate:1",
    " the:1",
    " dog:1",
    " cheese:1"
  ]
}
```

Doc2的计数： 
```bash
Doc 2 {
  "results": [
    " the:2",
    " house:1",
    " in:1",
    " mouse:1",
    " lovely:1",
    " was:1"
  ]
}
```

Doc3的计数： 
```bash
Doc 3 {
  "results": [
    " the:2",
    " in:1",
    " jungle:1",
    " mouse:1",
    " dead:1",
    " was:1"
  ]
}
```

我们将整个集合的计数称为`CollectionCount`，将单个文档称为`DocumentCount`。现在，如果你看一下Doc1中单词`the` 的概率，它是`1/7 = 0.143`，它的计算方法如下：

<img src="http://latex.codecogs.com/gif.latex?P(the)_{Doc1} = \frac {DocumentCount_{Doc1}(the)} { OverallWordCount_{Doc1} }">

单词`the`在Doc1中出现一次，总字数为7. Doc1 中`mouse`的概率也是`1/7 = 0.143`，如下所示：

<img src="http://latex.codecogs.com/gif.latex?P(mouse)_{Doc1} = \frac {DocumentCount_{Doc1}(mouse)} { OverallWordCount_{Doc1} }">

通过查看Doc1，即使我们知道`mouse`这个词对于Doc1比`the`这个词更重要，但概率并不能准确地反映出来。系统会认为这两个词同样重要，因为`mouse`和`the`一词具有相同的计数和概率。解决这个问题的一种方法是给 `IDF` 对概率的影响，即在整个集合中出现频繁的单词应该比在整体上较少的文档中出现的单词受到更多的惩罚。实际上，可以通过使用`CollectionCount`使用整个Collection中的概率对单词的概率进行标准化来实现此目的。因此，使用此规范化策略，`mouse`和`the`的更新分数将按如下方式计算：

<img src="http://latex.codecogs.com/gif.latex?score(mouse)_{Doc1} = \frac {P_{Doc1}(mouse)} { p_{EntireCollection}(mouse) } = \frac {0.143} {0.136} = 1.05">

<img src="http://latex.codecogs.com/gif.latex?score(the)_{Doc1} = \frac {P_{Doc1}(the)} { p_{EntireCollection}(the) } = \frac {0.143} {0.227} = 0.63">
  
现在，即使`文档内`概率相同，`the`这个词的分数也低于`mouse`。请注意，此示例基于非常小的样本，因此相同的理论可能不适用于示例文档中的某些其他单词。但是，当您实际采用大样本时，数字将准确反映文档和文档集合中单词的典型分布。当您按照分数的降序对`words/n-gram`进行排名时，这种类型的规范化很有用。另外要注意，当样本非常大时，概率将变得非常小，因此利用概率日志总是更好，因为它使分析更容易。例如，您可以执行以下操作：

<img src="http://latex.codecogs.com/gif.latex?score(the)_{Doc1} = log_2 \left\{ { \frac {P_{Doc1}(the)} { p_{EntireCollection}(the)  }} \right\} = log_2 \left\{ \frac {0.143} {0.227} \right\}= -0.66">

不要被负号吓到，当按照分数的降序对单词进行排名时，这一切都有意义。





