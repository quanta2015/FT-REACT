# 语料库和向量空间

## 1.将字符串转化成向量

首先，我们从将文件中的内容转化成string开始

```
>>> from gensim import corpora
>>>
>>> documents = ["Human machine interface for lab abc computer applications",
>>>              "A survey of user opinion of computer system response time",
>>>              "The EPS user interface management system",
>>>              "System and human system engineering testing of EPS",
>>>              "Relation of user perceived response time to error measurement",
>>>              "The generation of random binary unordered trees",
>>>              "The intersection graph of paths in trees",
>>>              "Graph minors IV Widths of trees and well quasi ordering",
>>>              "Graph minors A survey"]
```

这是一个由九个句子组成的小型文集，每一个都由一个句子构成

首先，我们将每个句子令牌化（将每个句子变成其中每个单词组成的数组），将公共的单词（使用stoplist）和出现过一次的单词

```
>>> # remove common words and tokenize
>>> stoplist = set('for a of the and to in'.split())
>>> texts = [[word for word in document.lower().split() if word not in stoplist]
>>>          for document in documents]
>>>
>>> # remove words that appear only once
>>> from collections import defaultdict
>>> frequency = defaultdict(int)
>>> for text in texts:
>>>     for token in text:
>>>         frequency[token] += 1
>>>
>>> texts = [[token for token in text if frequency[token] > 1]
>>>          for text in texts]
>>>
>>> from pprint import pprint  # pretty-printer
>>> pprint(texts)
[['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]
```

每个人处理文本的方式可能不一样；在这里，我只是将句子的每个单词变成小写形式，再通过空格将句子分开。事实上，我用这种特别（简单和低效）的体系来模仿Deerwester et al.’s original LSA article的实验。[1](https://radimrehurek.com/gensim/tut1.html#id3)

处理文本的方式太多而且太依赖于应用和语言，所以我决定不使用任何接口来限制它们,而是使用从中提取的特征来来表示一个document,而不是用表面的字符串的形式：如何提取特征取决于你。接下里我会展示一种常见的、通常（称为词袋）的方式，但是记住不同的应用域会需要不同的特征，然后，像一直以来的那样，就是[garbage in,gabage out](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out)

为了将句子转化成向量，我们将使用一种名为词袋的句子。在下面的演示中，每一个句子都用一个向量表示，向量中的每一个元素都可表示一个问题和一个回答的对，像这样：

“How many times does the word system appear in the document? Once.”

system这个词在句子中出现了几次？ 一次。

用整数的id来表示问题是非常有利的。问题和id的映射称为字典：

```
>>> dictionary = corpora.Dictionary(texts)
>>> dictionary.save('/tmp/deerwester.dict')  # store the dictionary, for future reference
>>> print(dictionary)
Dictionary(12 unique tokens)
```

在这里，我们用[ gensim.corpora.dictionary.Dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary)类给文集中的每一个单词赋予一个唯一的id。我们使用这个方法遍历整个文本，统计单词的个数和相关的数据。最后，我们看见处理后的文集中有12个不同的单词，这意味着每个句子都会由12个数字组成（一个12维的向量）。如果想查看单词和他们的id之间的映射：

```
>>> print(dictionary.token2id)
{'minors': 11, 'graph': 10, 'system': 5, 'trees': 9, 'eps': 8, 'computer': 0,
'survey': 4, 'user': 7, 'human': 1, 'time': 6, 'interface': 2, 'response': 3}
```

事实上，将令牌化的句子转化成向量可以用这样的方式：

```
>>> new_doc = "Human computer interaction"
>>> new_vec = dictionary.doc2bow(new_doc.lower().split())
>>> print(new_vec)  # the word "interaction" does not appear in the dictionary and is ignored
[(0, 1), (1, 1)]
```

doc2bow函数只会统计每个不同的单词的出现次数，将单词转化成对应的整数id并返回一个稀疏向量。稀疏向量[(0,1),(1,1)]表示：在句子"Human computer interaction",computer(id 0)和human(id 1)出现了1次；字典里的其他10个单词没有出现。

```
>>> corpus = [dictionary.doc2bow(text) for text in texts]
>>> corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  # store to disk, for later use
>>> print(corpus)
[(0, 1), (1, 1), (2, 1)]
[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]
[(2, 1), (5, 1), (7, 1), (8, 1)]
[(1, 1), (5, 2), (8, 1)]
[(3, 1), (6, 1), (7, 1)]
[(9, 1)]
[(9, 1), (10, 1)]
[(9, 1), (10, 1), (11, 1)]
[(4, 1), (10, 1), (11, 1)]
```

到现在我们应该清楚一个特征向量中id=10的值代表这样一个问题："graph这个词在句子中出现了多少次"，前6个句子的答案是0，而剩下的三个是1。事实上，我们获得了和[Quick Example](https://radimrehurek.com/gensim/tutorial.html#first-example)中一样的文集的向量。

## 文集流-一个处理一个文件

