title: 数据分析
theme: light


<style>
.m-cnn { width: 600px; margin: 10px; }
.m-lr { width: 800px; margin: 10px; }
</style>


[slide]
# 数据预处理


# dot 
矩阵相乘

[slide]
# tabular 数据
常见的表格数据有 excel/csv/tsv 等，通过数据库和文件集合也可以称为表格数据。在机器学习中，表格的数据行称为记录，而列被称为 `特征` ，有两种类型 `Measured` 和 `Calculated`。而数据类型同样也有两种，即 `categoriacal` 和 `continuous` 。机器学习的本质其实就是特征学习。

# pandas 库
Pandas 是python的一个数据分析包，是为了解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的函数和方法。


# 读csv数据 - read_csv
```python
import pandas  as pd
dataset = pd.read_csv('data.csv')
type(df) # pandas.core.frame.DataFrame
```

# head
`head` 返回头部数据  默认5条
```python
df.head()

   PassengerId  Survived  Pclass    ...        Fare Cabin  Embarked
0            1         0       3    ...      7.2500   NaN         S
1            2         1       1    ...     71.2833   C85         C
2            3         1       3    ...      7.9250   NaN         S
3            4         1       1    ...     53.1000  C123         S
4            5         0       3    ...      8.0500   NaN         S
```

# info
`info` 返回数据表的信息，包括数据数量、是否为空和数据类型
```python
df.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
PassengerId    891 non-null int64
Survived       891 non-null int64
Pclass         891 non-null int64
Name           891 non-null object
Sex            891 non-null object
Age            714 non-null float64
SibSp          891 non-null int64
Parch          891 non-null int64
Ticket         891 non-null object
Fare           891 non-null float64
Cabin          204 non-null object
Embarked       889 non-null object
dtypes: float64(2), int64(5), object(5)
memory usage: 83.6+ KB
```

# describe
describe 生成描述性统计数据，总结数据集分布的集中趋势，分散和形状，不包括 NaN值。

```python
df.describe()

       PassengerId    Survived      Pclass         Age       SibSp       Parch        Fare
count   891.000000  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000
mean    446.000000    0.383838    2.308642   29.699118    0.523008    0.381594   32.204208
std     257.353842    0.486592    0.836071   14.526497    1.102743    0.806057   49.693429
min       1.000000    0.000000    1.000000    0.420000    0.000000    0.000000    0.000000
25%     223.500000    0.000000    2.000000   20.125000    0.000000    0.000000    7.910400
50%     446.000000    0.000000    3.000000   28.000000    0.000000    0.000000   14.454200
75%     668.500000    1.000000    3.000000   38.000000    1.000000    0.000000   31.000000
max     891.000000    1.000000    3.000000   80.000000    8.000000    6.000000  512.329200
```



# 切割数据 - iloc
```python
# 读取全部列 除了最后一列
X = dataset.iloc[:, :-1].values
# 读取最后一列
y = dataset.iloc[:, -1].values
```


# 类别变量向量化 - get_dummies
```python
df = pd.read_csv('../data/weight-height.csv')
df.head()

#     Gender  Height      Weight
# 0   Male    73.847017   241.893563
# 1   Male    68.781904   162.310473
# 2   Male    74.110105   212.740856
# 3   Female  71.730978   220.042470
# 4   Female  69.881796   206.349801

pd.get_dummies(df['Gender'], prefix='Gender').head()

#     Gender_Female   Gender_Male
# 0   0               1
# 1   0               1
# 2   0               1
# 3   0               1
# 4   0               1
```


# 缩放 - MinMaxScaler
```python
from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
df['Weight_mms'] = mms.fit_transform(df[['Weight']])
df['Height_mms'] = mms.fit_transform(df[['Height']])
df.describe().round(2)

#         Height      Weight      Weight_mms  Height_mms
# count   10000.00    10000.00    10000.00    10000.00
# mean    66.37       161.44      0.47        0.49
# std     3.85        32.11       0.16        0.16
# min     54.26       64.70       0.00        0.00
# 25%     63.51       135.82      0.35        0.37
# 50%     66.32       161.21      0.47        0.49
# 75%     69.17       187.17      0.60        0.60
# max     79.00       269.99      1.00        1.00
```

# 标准化 - StandardScaler 
```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
df['Weight_ss'] = ss.fit_transform(df[['Weight']])
df['Height_ss'] = ss.fit_transform(df[['Height']])
df.describe().round(2)

#         Height      Weight      Weight_mms  Height_mms  Weight_ss   Height_ss
# count   10000.00    10000.00    10000.00    10000.00    10000.00    10000.00
# mean    66.37       161.44      0.47        0.49        -0.00       -0.00
# std     3.85        32.11       0.16        0.16        1.00        1.00
# min     54.26       64.70       0.00        0.00        -3.01       -3.15
# 25%     63.51       135.82      0.35        0.37        -0.80       -0.74
# 50%     66.32       161.21      0.47        0.49        -0.01       -0.01
# 75%     69.17       187.17      0.60        0.60        0.80        0.73
# max     79.00       269.99      1.00        1.00        3.38        3.28
```



# 处理缺失值 - Imputer

| Country | Age | Salary | Purchased |
|---------|-----|--------|-----------|
| France | 44 | 72000 | No |
| Spain | 27 | 48000 | Yes |
| Germany | 30 | 54000 | No |
| Spain | 38 | 61000 | No |
| Germany | 40 |  | Yes |
| France | 35 | 58000 | Yes |
| Spain |  | 52000 | No |
| France | 48 | 79000 | Yes |
| Germany | 50 | 83000 | No |
| France | 37 | 67000 | Yes |


```python
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)
imputer = imputer.fit(X[:, 1:3])
X[:, 1:3] = imputer.transform(X[:, 1:3])
```


# 标签化 - LabelEncoder 

| Country | Age | Salary | Purchased |
|----|-------|------|-----|
| France | 44.0 | 72000.0 | No |
| Spain | 27.0 | 48000.0 | Yes |
| Germany | 30.0 | 54000.0 | No |
| Spain | 38.0 | 61000.0 | No |
| Germany | 40.0 | 63777.7| Yes |
| France | 35.0 | 58000.0 | Yes |
| Spain | 38.7 | 52000.0 | No |
| France | 48.0 | 79000.0 | Yes |
| Germany | 50.0 | 83000.0 | No |
| France | 37.0 | 67000.0 | Yes |

```python
from sklearn.preprocessing import LabelEncoder
LabelEncoder_X = LabelEncoder()
X[:, 0] = LabelEncoder_X.fit_transform(X[:, 0])
X[:, -1] = labelencoder_X.fit_transform(X[:, -1])
```

| Country | Age | Salary | Purchased |
|----|-------|------|-----|
| 0 | 44.0 | 72000.0 | 0 |
| 2 | 27.0 | 48000.0 | 1 |
| 1 | 30.0 | 54000.0 | 0 |
| 2 | 38.0 | 61000.0 | 0 |
| 1 | 40.0 | 63777.7 | 1 |
| 0 | 35.0 | 58000.0 | 1 |
| 2 | 38.7 | 52000.0 | 0 |
| 0 | 48.0 | 79000.0 | 1 |
| 1 | 50.0 | 83000.0 | 0 |
| 0 | 37.0 | 67000.0 | 1 |



# 独热化 - OneHotEncoder 
```python
from sklearn.preprocessing import OneHotEncoder
onehotencoder = OneHotEncoder(categorical_features = [0])
X = onehotencoder.fit_transform(X).toarray()
```

| France | Germany |Spain |Age | Salary | Purchased |
|---|---|---|---------|---------|---|
| 1 | 0 | 0 | 44 | 72000 | 0 |
| 0 | 0 | 1 | 27 | 48000 | 1 |
| 0 | 1 | 0 | 30 | 54000 | 0 |
| 0 | 0 | 1 | 38 | 61000 | 0 |
| 0 | 1 | 0 | 40 | 63777.8 | 1 |
| 1 | 0 | 0 | 35 | 58000 | 1 |
| 0 | 0 | 1 | 38.7| 52000 | 0 |
| 1 | 0 | 0 | 48 | 79000 | 1 |
| 0 | 1 | 0 | 50 | 83000 | 0 |
| 1 | 0 | 0 | 37 | 67000 | 1 |





# 数据集划分 - train_test_split

```python
sklearn.model_selection.train_test_split(*arrays, **options)
# 主要参数说明: 
# - *arrays: 可以是列表、numpy数组、scipy稀疏矩阵或pandas的数据框
# - test_size: 可以为浮点、整数或None，默认为None
#   ① 若为浮点时，表示测试集占总样本的百分比
#   ② 若为整数时，表示测试样本样本数
#   ③ 若为None时，test size自动设置成0.25
# - train_size: 可以为浮点、整数或None，默认为None
#   ① 若为浮点时，表示训练集占总样本的百分比
#   ② 若为整数时，表示训练样本的样本数
#   ③ 若为None时，train_size自动被设置成0.75
# - random_state: 可以为整数、RandomState实例或None，默认为None
#   ① 若为None时，每次生成的数据都是随机，可能不一样
#   ② 若为整数时，每次生成的数据都相同
# - stratify: 可以为类似数组或None
#   ① 若为None时，划分出来的测试集或训练集中，其类标签的比例也是随机的
#   ② 若不为None时，划分出来的测试集或训练集中，其类标签的比例同输入的数组中类标签的比例相同，可以用于处理不均衡的数据集
```

```
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)
```


# K折交叉验证 - KFold
思路：将训练/测试数据集划分n_splits个互斥子集，每次用其中一个子集当作验证集，剩下的n_splits-1个作为训练集，进行n_splits次训练和测试，得到n_splits个结果.

注意点：对于不能均等份的数据集，其前`n_samples % n_splits`子集拥有`n_samples // n_splits + 1`个样本，其余子集都只有`n_samples // n_splits`样本

```python
sklearn.model_selection.KFold(n_splits=3, shuffle=False, random_state=None)

# 参数说明：
# - n_splits：表示划分几等份
# - shuffle：在每次划分时，是否进行洗牌
#   ① 若为Falses时，其效果等同于random_state等于整数，每次划分的结果相同
#   ② 若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的
# - random_state：随机种子数
```

```python
from sklearn.model_selection import KFold
import numpy as np
X = np.arange(6)
kf = KFold(n_splits=3,shuffle=True)
for train_index , test_index in kf.split(X):
    print('train_index:%s , test_index: %s ' %(train_index,test_index))

# Out
train_index:[0 2 4 5] , test_index: [1 3] 
train_index:[0 1 3 5] , test_index: [2 4] 
train_index:[1 2 3 4] , test_index: [0 5] 
```


# 性能评估 - sklearn.metrics.confusion_matrix
```python
sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)
# 参数说明
# y_true: 真实因变量值
# y_pred: 预测因变量值
# labels: 矩阵的标签列表索引顺序
# sample_weight: 样本权重

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
# Out[xx]:  array([[65,  3], [ 8, 24]])
```




# KNN - k-Nearest Neighbor
所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例, 如果K个实例的多数属于某个类P，则输入实例属于这个类P中。


![](../../img/deep/knn01.png)

图形中有2种已知的图形，分别是蓝色的方块和红色的三角，绿色的未知图形。通过K近邻算法去判断未知的绿色图形是什么，取决于它周围距离最近的图形。
比如把K近邻的近邻参数设置成3(图中实线圆圈)，那么未知的绿色图形就是三角。如果近邻参数设置成5(图中虚线圆圈)，那么未知的绿色图形就是方块。

# KNN - 案例

自己在美国有一套房子想要在Airbnb平台上进行短租，但是不知道自己的房子能租多少钱？设置多少钱才是最合理的呢？
好吧，开始前我先说明一下我的房子情况: 我的房子可以容纳6名旅客，有3个卧室，厕所每个卧室都有，有3张床，我想最短租1天我还是能接受的，最多租30天好像就差不多了。

```python
import pandas as pd
from sklearn.neighbors import KNeighborsRegressor 
from sklearn.metrics import mean_squared_error 

dc_listings=pd.read_csv('listings.csv') 
features = ['accommodates','bedrooms','bathrooms','beds','price','minimum_nights','maximum_nights','number_of_reviews'] 
dc_listings = dc_listings[features] 
dc_listings.head()
dc_listings['price']=dc_listings.price.str.replace("\$|,",'').astype(float) 
dc_listings=dc_listings.dropna()  # 过滤缺失数据
normalized_listings = dc_listings # 复制一下数据
norm_train_df=normalized_listings.copy().iloc[0:2792] # 创建训练集训练集取2792个样本
norm_test_df=normalized_listings.copy().iloc[2792:] # 创建测试集取879个样本

# K近邻模型
cols = ['accommodates','bedrooms','bathrooms','beds','minimum_nights','maximum_nights','number_of_reviews'] # 选择测试集的训练的列
knn = KNeighborsRegressor(10) # 模型近邻值手动设置成10,其他为默认参数
knn.fit(norm_train_df[cols], norm_train_df['price']) # X放入训练集数据，Y放入目标输出数据
two_features_predictions = knn.predict(norm_test_df[cols]) # 输出测试集结果

# 模型效果验证
two_features_mse = mean_squared_error(norm_test_df['price'], two_features_predictions)
two_features_rmse = two_features_mse ** (1/2)
print(two_features_rmse) # 输出模型验证结果，根据结果调整近邻参数


# 数字代表我房子与数量列对应的名称，容纳旅客数=6，卧室=3，厕所=3，床=3，最少租=1，最多租=30，评论=0
print(knn.predict([[6,3,3,3,1,30,0]])) #设置自己房子的信息，预测出对应的价格
```

# NaiveBayes - 朴素贝叶斯算法
贝叶斯算法是英国数学家贝叶斯(约1701-1761)Thomas Bayes，生前提出为解决“逆概”问题写的一篇文章。“逆概”就是逆向概率，有逆向概率就会有正向概率。

- 正向概率：假设袋子里面有两种球N个白球和M个黑球，你伸手进去摸一个，摸出黑球的概率有多大？正向算概率我们就用黑球的个数除以球的总数就等于摸出黑球的概率。
- 逆向概率：如果我们事先不知道袋子里有多少个球，也不知道黑白球的比例。而是闭着眼睛随便摸一个（或者摸多个）球，观察这些取出来的球的颜色之后。那么我们可以就此对袋子里面的黑白球的比例作出推测。
现实世界本身就是不确定的，人类的观察能力有限。我们日常所观察到的只是事物表面上的结果，因此我们需要提供一个猜测。这个就是贝叶斯需要解决的问题。



# LinearRegression - 线性回归算法
线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。

<img src="../../img/cloud/deep/lr00.png"  class="m-cnn">


回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析

<img src="../../img/cloud/deep/lr01.png"  class="m-lr">

定义：线性回归在假设特证满足线性关系，根据给定的训练数据训练一个模型，并用此模型进行预测。

为了了解这个定义，我们先举个简单的例子；我们假设一个线性方程 Y=2x+30000, x变量为工作经验，y代表为薪水；新员工的工作经验是0其薪水是30000,而工作1年后，其增加的工资是10000, 通过这个模型，我们可以对每个员工的工资给出评估标准；

<img src="../../img/cloud/deep/lr02.png"  class="m-lr">

# 损失函数 cost of lossing
为了拟合出最优化的回归方程，要求sum(y-y^)2最小。损失函数包括2种：

- 总误差
- 均方误差

<img src="../../img/cloud/deep/lr03.png"  class="m-lr">




# 实例 - 一元线性回归算法
已知房屋面积与价格数据，通过用面积预测价格。

```python
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression 

# 创建一组7行2列的数据，square_feet为房屋面积，price为对应价格
data=pd.DataFrame({'square':[150,200,250,300,350,400,600],
                   'price':[5450,6850,8750,9650,10450,13450,16450]})

# 将数据转化为一个1维矩阵
data_train=np.array(data['square']).reshape(data['square'].shape[0],1)
data_test=data['price']

# 创建线性回归模型
regr = LinearRegression() 
regr.fit(data_train,data_test)
price = regr.predict(268.5)
print(price) 
# 查看拟合准确率情况,这里的检验是 R^2 ，趋近于1模型拟合越好
print(regr.score(data_train,data_test))
# 预测结果：268.5平的房子价格为8833.54， R^2 =0.967

#画散点图看实际面积和价格的分布情况
plt.scatter(data['square'],data['price']) 
#画拟合面积与价格的线型图
plt.plot(data['square'],regr.predict(np.array(data['square']).reshape(data['square'].shape[0],1)),color='red') 
```


# 实例 - 二元线性回归算法
已知房屋面积、卧室数量与价格的数据，通过用面积预测价格。
```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression 
data=pd.DataFrame({'square':[150,200,230,255,260,340,700,720,790,850,900,950,1000],'price':[5450,6850,8750,9650,10450,13450,16450,16930,17200,17943,18320,18412,18900],'bedrooms':[2,2,3,4,4,5,6,6,6,7,7,8,9]})

data_train=np.array(data[['square','bedrooms']]).reshape(len(data),2)
data_test=np.array(data['price']).reshape(len(data),1)

regr=LinearRegression() 
regr.fit(data_train,data_test)
price = regr.predict([[268.5,3]])
print(price) # 预测的结果：9073
```



# 逻辑回归 logistic regression
通过一个网络购物范例来说明，数据包括2个特征值：人们在网站上购物的时间和是否会购买。
<img src="../../img/cloud/deep/lr04.png"  class="m-lr">

类似，也可以通过问卷调查系统来说明，x轴是调查人群的年龄，y轴是是否会填写问卷，通过模型来说明年龄段多少之间是最适合做为目标客户的。
<img src="../../img/cloud/deep/lr05.png"  class="m-lr">

# 过度拟合 overfitting
对于机器来说，在使用学习算法学习数据的特征的时候，样本数据的特征可以分为局部特征和全局特征，全局特征就是任何你想学习的那个概念所对应的数据都具备的特征，而局部特征则是你用来训练机器的样本里头的数据专有的特征.

<img src="../../img/cloud/deep/lr06.png"  class="m-lr">

在学习算法的作用下，机器在学习过程中是无法区别局部特征和全局特征的，于是机器在完成学习后，除了学习到了数据的全局特征，也可能习得一部分局部特征，而习得的局部特征比重越多，那么新样本中不具有这些局部特征但具有所有全局特征的样本也越多，于是机器无法正确识别符合概念定义的“正确”样本的几率也会上升，也就是所谓的“泛化性”变差，这是过拟合会造成的最大问题.
所谓过拟合，就是指把学习进行的太彻底，把样本数据的所有特征几乎都习得了，于是机器学到了过多的局部特征，过多的由于噪声带来的假特征，造成模型的“泛化性”和识别正确率几乎达到谷点，于是你用你的机器识别新的样本的时候会发现就没几个是正确识别的.

解决过拟合的方法，其基本原理就是限制机器的学习，使机器学习特征时学得不那么彻底，因此这样就可以降低机器学到局部特征和错误特征的几率，使得识别正确率得到优化。一般来说需要避免下面几点：

- 合理分配训练和测试的比例
- 随机分布数据集
- 测试集合不要过小
- 训练集合不要过小


# 交叉验证 cross-validation
交叉验证就是重复使用数据，把样本数据进行多次切分，组合为不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。　

那么什么时候才需要交叉验证呢？交叉验证用在数据不是很充足的时候。比如在我日常项目里面，对于普通适中问题，如果数据样本量小于一万条，我们就会采用交叉验证来训练优化选择模型。
<img src="../../img/cloud/deep/lr07.png"  class="m-lr">


# 混淆矩阵（Confusion Matrix）
混淆矩阵的每一列代表了预测类别，每一列的总数表示预测为该类别的数据的数目；每一行代表了数据的真实归属类别，每一行的数据总数表示该类别的数据实例的数目。每一列中的数值表示真实数据被预测为该类的数目：如下图，第一行第一列中的43表示有43个实际归属第一类的实例被预测为第一类，同理，第一行第二列的2表示有2个实际归属为第一类的实例被错误预测为第二类。

| | 预测类1 | 预测类2 | 预测类3 |
|---|-------|---------|-----------|
| 实际类1   | 43   |   2    |   0  |
| 实际类2  |  5    |   45   |   1  |
| 实际类3  |  2    |   3    |   49 |


# 多类问题
常见的数据分类中，最容易出现的是两种类型： `互斥类` 和 `非互斥类`。 

<img src="../../img/cloud/deep/lr08.png"  class="m-cnn">

对于这两种类型，在深度学习模型中，必须通过不同的激活函数来处理。
<img src="../../img/cloud/deep/lr09.png"  class="m-cnn">



对于`非互斥类`的计算，由于在最后的输出结果中需要有多维向量，因此一般使用 `sigmoid` 函数。
<img src="../../img/cloud/deep/lr10.png"  class="m-cnn">


对于`互斥类`的计算，由于在最后的输出结果中只有一组互斥向量，因此一般使用 `softmax` 函数。
<img src="../../img/cloud/deep/lr11.png"  class="m-cnn">


