title: 数据分析范例
theme: light

<style>
.m-data { width: 400px; margin: 10px; }
.m-cnn { width: 600px; margin: 10px; }
.m-lr { width: 800px; margin: 10px; }
</style>

### 查看数据

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df = pd.read_csv('../data/weight-height.csv')
df.head()

#    Gender  Height  Weight
# 0   Male    73.847017   241.893563
# 1   Male    68.781904   162.310473
# 2   Male    74.110105   212.740856
# 3   Male    71.730978   220.042470
# 4   Male    69.881796   206.349801
```

# 基本图像
```python
df.plot(kind='scatter',
            x='Height',
            y='Weight')
plt.plot([55,78],[75,250],color='red',linewidth=3)

def line(x, w=0, b=0):
   return  x*w + b

x = np.linspace(55,80,100)
y = line(x, w=0, b=0)
```
<img src="../../img/cloud/exp/data01.png"  class="m-data">

# 分类图像
```python
import matplotlib.pyplot as plt
males = df[df['Gender']=='Male']
females = df.query('Gender=="Female"')
fig, ax =plt.subplots()
males.plot(kind='scatter',x='Height',y='Weight', 
           ax=ax,color='blue', alpha=.3)
females.plot(kind='scatter',x='Height',y='Weight', 
           ax=ax,color='red', alpha=.3)
```
<img src="../../img/cloud/exp/data02.png"  class="m-data">

# 计算MSE
```python
def line(x, w=0, b=0):
   return  x*w + b
def mean_squared_error(y_true, y_pred):
    s = (y_true-y_pred)**2
    return s.mean()

x = np.linspace(55,80,100)
y = line(x, w=0, b=0)
df.plot(kind='scatter',
            x='Height',
            y='Weight')
plt.plot(x,y,color='red',linewidth=3)

X = df[['Height']].values
y_true = df['Weight'].values
y_pred = line(X)
mse = mean_squared_error(y_true,y_pred)
print(mse) # 27093.83757456164
```
<img src="../../img/cloud/exp/data03.png"  class="m-data">

# 损失函数
```python
plt.figure(figsize=(10, 3))

# plt.subplot('行','列','编号')
ax1 = plt.subplot(121)
df.plot(kind='scatter',
        x='Height',
        y='Weight',
        title='Weight and Height in adults', ax=ax1)

# 预测梯度
bbs = np.array([-100, -50, 0, 50, 100, 150])
mses = [] 
for b in bbs:
    y_pred = line(X, w=2, b=b)
    mse = mean_squared_error(y_true, y_pred)
    mses.append(mse)
    plt.plot(X, y_pred)

# 损失函数
ax2 = plt.subplot(122)
plt.plot(bbs, mses, 'o-')
plt.title('Cost as a function of b')
plt.xlabel('b')
```
<img src="../../img/cloud/exp/data04.png"  class="m-cnn">

# 使用神经网络求解
```python
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam, SGD

model = Sequential()
model.add(Dense(1, input_shape=(1,)))
model.compile(Adam(lr=0.8), 'mean_squared_error')
model.fit(X, y_true, epochs=40)
y_pred = model.predict(X)
df.plot(kind='scatter',
        x='Height',
        y='Weight',
        title='Weight and Height in adults')
plt.plot(X, y_pred, color='red')
W, B = model.get_weights()
print('%s %s'%(W[0][0],B[0]))  # 7.8429036 -347.9816
```
<img src="../../img/cloud/exp/data05.png"  class="m-data">


# 评估模型
```python
from sklearn.metrics import r2_score
print("The R2 score is {:0.3f}".format(r2_score(y_true, y_pred))) # The R2 score is 0.736

# 分割训练测试集合
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y_true,
                                                    test_size=0.2)
W[0, 0] = 0.0
B[0] = 0.0
model.set_weights((W, B))
model.fit(X_train, y_train, epochs=50, verbose=0)
y_train_pred = model.predict(X_train).ravel()
y_test_pred = model.predict(X_test).ravel()

from sklearn.metrics import mean_squared_error as mse
print("The Mean Squared Error on the Train set is:\t{:0.1f}".format(mse(y_train, y_train_pred)))
print("The Mean Squared Error on the Test set is:\t{:0.1f}".format(mse(y_test, y_test_pred)))
# The Mean Squared Error on the Train set is:   186.1
# The Mean Squared Error on the Test set is:  179.7
print("The R2 score on the Train set is:\t{:0.3f}".format(r2_score(y_train, y_train_pred)))
print("The R2 score on the Test set is:\t{:0.3f}".format(r2_score(y_test, y_test_pred)))
# The R2 score on the Train set is: 0.820
# The R2 score on the Test set is:    0.824
```

# 逻辑回归
```python
df = pd.read_csv('../data/user_visit_duration.csv')
df.head()
#     Time (min)  Buy
# 0   2.000000    0
# 1   0.683333    0
# 2   3.216667    1
# 3   0.900000    0
# 4   1.533333    1

df.plot(kind='scatter', x='Time (min)', y='Buy')
```
<img src="../../img/cloud/exp/data06.png"  class="m-data">

```python
X = df[['Time (min)']].values
y = df['Buy'].values

model = Sequential()
model.add(Dense(1, input_shape=(1,), activation='sigmoid'))
model.compile(SGD(lr=0.5), 'binary_crossentropy', metrics=['accuracy'])
model.fit(X, y, epochs=25)
ax = df.plot(kind='scatter', x='Time (min)', y='Buy',
             title='Purchase behavior VS time spent on site')

temp = np.linspace(0, 4)
ax.plot(temp, model.predict(temp), color='orange')
plt.legend(['model', 'data'])
```
<img src="../../img/cloud/exp/data07.png"  class="m-data">

```python
temp_class = model.predict(temp) > 0.5
ax = df.plot(kind='scatter', x='Time (min)', y='Buy',
             title='Purchase behavior VS time spent on site')

temp = np.linspace(0, 4)
ax.plot(temp, temp_class, color='orange')
plt.legend(['model', 'data'])
```
<img src="../../img/cloud/exp/data08.png"  class="m-data">

```python
y_pred = model.predict(X)
y_class_pred = y_pred > 0.5
from sklearn.metrics import accuracy_score
print("The accuracy score is {:0.3f}".format(accuracy_score(y, y_class_pred)))
# The accuracy score is 0.830
```

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
params = model.get_weights()
params = [np.zeros(w.shape) for w in params]
model.set_weights(params)
print("The accuracy score is {:0.3f}".format(accuracy_score(y, model.predict(X) > 0.5)))
# The accuracy score is 0.500
model.fit(X_train, y_train, epochs=25, verbose=0)
print("The train accuracy score is {:0.3f}".format(accuracy_score(y_train, model.predict(X_train) > 0.5)))
print("The test accuracy score is {:0.3f}".format(accuracy_score(y_test, model.predict(X_test) > 0.5)))
# The train accuracy score is 0.838
# The test accuracy score is 0.800
```


# 交叉验证
```python
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score, KFold

def build_logistic_regression_model():
    model = Sequential()
    model.add(Dense(1, input_shape=(1,), activation='sigmoid'))
    model.compile(SGD(lr=0.5),
                  'binary_crossentropy',
                  metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=build_logistic_regression_model,
                        epochs=25,
                        verbose=0)
cv = KFold(3, shuffle=True)
scores = cross_val_score(model, X, y, cv=cv)
print(scores) 
# array([0.79411765, 0.75757576, 0.84848485])
print("The cross validation accuracy is {:0.4f} ± {:0.4f}".format(scores.mean(), scores.std())) 
# The cross validation accuracy is 0.8001 ± 0.0374
```

# 混淆矩阵
```python
from sklearn.metrics import confusion_matrix
confusion_matrix(y, y_class_pred)
# array([[40, 10],
#       [ 7, 43]])

def pretty_confusion_matrix(y_true, y_pred, labels=["False", "True"]):
    cm = confusion_matrix(y_true, y_pred)
    pred_labels = ['Predicted '+ l for l in labels]
    df = pd.DataFrame(cm, index=labels, columns=pred_labels)
    return df
pretty_confusion_matrix(y, y_class_pred, ['Not Buy', 'Buy'])

#         Predicted Not Buy   Predicted Buy
# Not Buy                40   10
# Buy                     7   43

from sklearn.metrics import precision_score, recall_score, f1_score
print("Precision:\t{:0.3f}".format(precision_score(y, y_class_pred)))
print("Recall:  \t{:0.3f}".format(recall_score(y, y_class_pred)))
print("F1 Score:\t{:0.3f}".format(f1_score(y, y_class_pred)))

# Precision:    0.811
# Recall:     0.860
# F1 Score:   0.835

from sklearn.metrics import classification_report
print(classification_report(y, y_class_pred))

#               precision    recall  f1-score   support
# 
#            0       0.85      0.80      0.82        50
#            1       0.81      0.86      0.83        50
# 
#    micro avg       0.83      0.83      0.83       100
#    macro avg       0.83      0.83      0.83       100
# weighted avg       0.83      0.83      0.83       100

```


# 分类范例

#### 1. 原始数据类
```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=1000, noise=0.1, random_state=0)
plt.plot(X[y==0, 0], X[y==0, 1], 'ob', alpha=0.5)
plt.plot(X[y==1, 0], X[y==1, 1], 'xr', alpha=0.5)
plt.legend(['0', '1'])
```
<img src="../../img/cloud/exp/data09.png"  class="m-data">


#### 2. 浅层模型
```python
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD, Adam

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.3,
                                                    random_state=42)
model = Sequential()
model.add(Dense(1, input_shape=(2,), activation='sigmoid'))
model.compile(Adam(lr=0.05), 'binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=200, verbose=0)
results = model.evaluate(X_test, y_test) # [0.31866680781046547, 0.8433333341280619]
print("The Accuracy score on the Train set is:\t{:0.3f}".format(results[1]))
# The Accuracy score on the Train set is: 0.843

def plot_decision_boundary(model, X, y):
    amin, bmin = X.min(axis=0) - 0.1
    amax, bmax = X.max(axis=0) + 0.1
    hticks = np.linspace(amin, amax, 101)
    vticks = np.linspace(bmin, bmax, 101)
    aa, bb = np.meshgrid(hticks, vticks)
    ab = np.c_[aa.ravel(), bb.ravel()]
    c = model.predict(ab)
    cc = c.reshape(aa.shape)
    plt.figure(figsize=(12, 8))
    plt.contourf(aa, bb, cc, cmap='bwr', alpha=0.2)
    plt.plot(X[y==0, 0], X[y==0, 1], 'ob', alpha=0.5)
    plt.plot(X[y==1, 0], X[y==1, 1], 'xr', alpha=0.5)
    plt.legend(['0', '1'])

plot_decision_boundary(model, X, y)
```
<img src="../../img/cloud/exp/data10.png"  class="m-data">

#### 3. 深层模型
```python
model = Sequential()
model.add(Dense(4, input_shape=(2,), activation='tanh'))
model.add(Dense(2, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
model.compile(Adam(lr=0.05), 'binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=100, verbose=0)
model.evaluate(X_test, y_test)
from sklearn.metrics import accuracy_score, confusion_matrix
y_train_pred = model.predict_classes(X_train)
y_test_pred = model.predict_classes(X_test)
print("The Accuracy score on the Train set is:\t{:0.3f}".format(accuracy_score(y_train, y_train_pred)))
print("The Accuracy score on the Test set is:\t{:0.3f}".format(accuracy_score(y_test, y_test_pred)))
plot_decision_boundary(model, X, y)
```
<img src="../../img/cloud/exp/data11.png"  class="m-data">



# 多类别分类

```python
df = pd.read_csv('../data/iris.csv')
import seaborn as sns
sns.pairplot(df, hue="species")
```
<img src="../../img/cloud/exp/data12.png"  class="m-cnn">

```python
df.head()
#   sepal_length  sepal_width petal_length  petal_width species
# 0          5.1          3.5          1.4          0.2 setosa
# 1          4.9          3.0          1.4          0.2 setosa
# 2          4.7          3.2          1.3          0.2 setosa
# 3          4.6          3.1          1.5          0.2 setosa
# 4          5.0          3.6          1.4          0.2 setosa

X = df.drop('species', axis=1)
target_names = df['species'].unique()
# array(['setosa', 'versicolor', 'virginica'], dtype=object)

target_dict = {n:i for i, n in enumerate(target_names)}
# {'setosa': 0, 'versicolor': 1, 'virginica': 2}

y= df['species'].map(target_dict)

from keras.utils.np_utils import to_categorical
y_cat = to_categorical(y)

X_train, X_test, y_train, y_test = train_test_split(X.values, y_cat,
                                                    test_size=0.2)
model = Sequential()
model.add(Dense(3, input_shape=(4,), activation='softmax'))
model.compile(Adam(lr=0.1),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(X_train, y_train, epochs=20, validation_split=0.1)
y_pred = model.predict(X_test)
y_test_class = np.argmax(y_test, axis=1)
y_pred_class = np.argmax(y_pred, axis=1)
from sklearn.metrics import classification_report
print(classification_report(y_test_class, y_pred_class))

#               precision    recall  f1-score   support
# 
#            0       1.00      1.00      1.00         6
#            1       0.90      1.00      0.95        19
#            2       1.00      0.60      0.75         5
# 
#    micro avg       0.93      0.93      0.93        30
#    macro avg       0.97      0.87      0.90        30
# weighted avg       0.94      0.93      0.93        30


confusion_matrix(y_test_class, y_pred_class)

# array([[ 6,  0,  0],
#        [ 0, 19,  0],
#        [ 0,  2,  3]])
```


# 学习率  Learning rate
```python
df = pd.read_csv('../data/banknotes.csv')
df.head()
#   variace skewness  curtosis  entropy class
# 0 3.62160 8.6661  -2.8073 -0.44699  0
# 1 4.54590 8.1674  -2.4586 -1.46210  0
# 2 3.86600 -2.6383 1.9242  0.10645 0
# 3 3.45660 9.5228  -4.0112 -3.59440  0
# 4 0.32924 -4.4552 4.5718  -0.98880  0

df['class'].value_counts()
# 0    762
# 1    610

import seaborn as sns
sns.pairplot(df, hue="class")
```
<img src="../../img/cloud/exp/data13.png"  class="m-data">

```python
from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import scale
X = scale(df.drop('class', axis=1).values)
y = df['class'].values
model = RandomForestClassifier()
cross_val_score(model, X, y)
# array([0.99563319, 0.99124726, 0.99343545])

# 建立逻辑回归模型
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.3,
                                                    random_state=42)
import keras.backend as K
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.optimizers import SGD
K.clear_session()
model = Sequential()
model.add(Dense(1, input_shape=(4,), activation='sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs = 10)
result = model.evaluate(X_test, y_test)
historydf = pd.DataFrame(history.history, index=history.epoch)
historydf.plot(ylim=(0,1))
plt.title("Test accuracy: {:3.1f} %".format(result[1]*100), fontsize=15)
# Text(0.5,1,'Test accuracy: 93.4 %')
```
<img src="../../img/cloud/exp/data14.png"  class="m-data">

```python
dflist = []
learning_rates = [0.01, 0.05, 0.1, 0.5]

for lr in learning_rates:
    K.clear_session()
    model = Sequential()
    model.add(Dense(1, input_shape=(4,), activation='sigmoid'))
    model.compile(loss='binary_crossentropy',
                  optimizer=SGD(lr=lr),
                  metrics=['accuracy'])
    h = model.fit(X_train, y_train, epochs=10,batch_size=16, verbose=0)
    dflist.append(pd.DataFrame(h.history, index=h.epoch))

historydf = pd.concat(dflist, axis=1)
historydf.head()
#   acc loss  acc loss  acc loss  acc loss
# 0 0.832292  0.447641  0.738542  0.488358  0.857292  0.427632  0.956250  0.# 213856
# 1 0.916667  0.386766  0.911458  0.327653  0.934375  0.259444  0.967708  0.# 113151
# 2 0.938542  0.348892  0.930208  0.270660  0.948958  0.208807  0.973958  0.# 089170
# 3 0.944792  0.323494  0.948958  0.237652  0.955208  0.178642  0.975000  0.# 076987
# 4 0.941667  0.305130  0.957292  0.214127  0.959375  0.157504  0.979167  0.# 069091

metrics_reported = dflist[0].columns
idx = pd.MultiIndex.from_product([learning_rates, metrics_reported],
                                 names=['learning_rate', 'metric'])
historydf.columns = idx
# learning_rate   0.01                0.05                0.10                0.50
# metric          acc       loss      acc       loss      acc       loss      acc       loss
#               0 0.832292  0.447641  0.738542  0.488358  0.857292  0.427632  0.956250  0.213856
#               1 0.916667  0.386766  0.911458  0.327653  0.934375  0.259444  0.967708  0.113151
#               2 0.938542  0.348892  0.930208  0.270660  0.948958  0.208807  0.973958  0.089170
#               3 0.944792  0.323494  0.948958  0.237652  0.955208  0.178642  0.975000  0.076987
#               4 0.941667  0.305130  0.957292  0.214127  0.959375  0.157504  0.979167  0.069091

ax = plt.subplot(211)
historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)
plt.title("Loss")
ax = plt.subplot(212)
historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)
plt.title("Accuracy")
plt.xlabel("Epochs")
plt.tight_layout()
```
<img src="../../img/cloud/exp/data15.png"  class="m-data">


# 批量数对比测试 - Batch Size
```python
dflist = []
batch_sizes = [16, 32, 64, 128]
for batch_size in batch_sizes:
    K.clear_session()
    model = Sequential()
    model.add(Dense(1, input_shape=(4,), activation='sigmoid'))
    model.compile(loss='binary_crossentropy',
                  optimizer='sgd',
                  metrics=['accuracy'])
    h = model.fit(X_train, y_train, epochs =10, batch_size=batch_size, verbose=0)
    dflist.append(pd.DataFrame(h.history, index=h.epoch))

historydf = pd.concat(dflist, axis=1)
metrics_reported = dflist[0].columns
idx = pd.MultiIndex.from_product([batch_sizes, metrics_reported],
                                 names=['batch_size', 'metric'])
historydf.columns = idx

# batch_size  16  32  64  128
# metric      acc       loss      acc       loss      acc       loss      acc loss
#           0 0.338542  1.118057  0.554167  0.703027  0.138542  1.123439  0.529167  0.793724
#           1 0.432292  0.853531  0.596875  0.635042  0.182292  1.073837  0.544792  0.766993
#           2 0.610417  0.678452  0.661458  0.577809  0.209375  1.028946  0.562500  0.741393
#           3 0.831250  0.571662  0.743750  0.529888  0.231250  0.988386  0.593750  0.717430
#           4 0.900000  0.505454  0.805208  0.489723  0.276042  0.951674  0.622917  0.694492

ax = plt.subplot(211)
historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)
plt.title("Loss")
ax = plt.subplot(212)
historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)
plt.title("Accuracy")
plt.xlabel("Epochs")
plt.tight_layout()
```
<img src="../../img/cloud/exp/data16.png"  class="m-cnn">


# 优化器对比测试 - Optimizers
```python
from keras.optimizers import SGD, Adam, Adagrad, RMSprop
dflist = []

optimizers = ['SGD(lr=0.01)',
              'SGD(lr=0.01, momentum=0.3)',
              'SGD(lr=0.01, momentum=0.3, nesterov=True)',  
              'Adam(lr=0.01)',
              'Adagrad(lr=0.01)',
              'RMSprop(lr=0.01)']
for opt_name in optimizers:
    K.clear_session()
    model = Sequential()
    model.add(Dense(1, input_shape=(4,), activation='sigmoid'))
    model.compile(loss='binary_crossentropy',
                  optimizer=eval(opt_name),
                  metrics=['accuracy'])
    h = model.fit(X_train, y_train, batch_size=16, epochs=5, verbose=0)
    dflist.append(pd.DataFrame(h.history, index=h.epoch))

historydf = pd.concat(dflist, axis=1)
metrics_reported = dflist[0].columns
idx = pd.MultiIndex.from_product([optimizers, metrics_reported],
                                 names=['optimizers', 'metric'])
historydf.columns = idx

plt.figure(figsize=(10, 10))
ax = plt.subplot(211)
historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)
plt.title("Loss")
ax = plt.subplot(212)
historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)
plt.title("Accuracy")
plt.xlabel("Epochs")
plt.tight_layout()
```
<img src="../../img/cloud/exp/data17.png"  class="m-cnn">

# 初始化测试对比 - Initialization
```python
dflist = []
initializers = ['zeros', 'uniform', 'normal',
                'he_normal', 'lecun_uniform']

for init in initializers:
    K.clear_session()
    model = Sequential()
    model.add(Dense(1, input_shape=(4,),
                    kernel_initializer=init,
                    activation='sigmoid'))
    model.compile(loss='binary_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])
    h = model.fit(X_train, y_train, batch_size=16, epochs=5, verbose=0)
    dflist.append(pd.DataFrame(h.history, index=h.epoch))

historydf = pd.concat(dflist, axis=1)
metrics_reported = dflist[0].columns
idx = pd.MultiIndex.from_product([initializers, metrics_reported],
                                 names=['initializers', 'metric'])
historydf.columns = idx

ax = plt.subplot(211)
historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)
plt.title("Loss")
ax = plt.subplot(212)
historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)
plt.title("Accuracy")
plt.xlabel("Epochs")
plt.tight_layout()
```
<img src="../../img/cloud/exp/data18.png"  class="m-cnn">

